## Введение

Этот проект представляет собой реализацию нескольких методов классификации, включая метод опорных векторов (SVC), логистическую регрессию и метод ближайших соседей (k-NN). Код написан на Python и демонстрирует базовые концепции машинного обучения, такие как обучение моделей, предсказания и визуализация результатов.

## Установка и подготовка окружения

### Требуемые библиотеки

- **numpy**
- **pandas**
- **matplotlib**

Установите их с помощью `pip`:

```bash
pip install numpy pandas matplotlib
```

### Подготовка данных

Для работы с кодом требуется набор данных в виде матрицы признаков (X) и целевого вектора (y), представленных в формате `pandas.DataFrame`.

---

## Описание методов классификации

### Метод опорных векторов (SVC1)

 **Класс** : `SVC1`

Классический линейный метод опорных векторов для бинарной классификации. Поддерживает:

* Предсказания (`predict`).
* Обучение методом градиентного спуска (`fit`).
* Оценку точности классификации (`accuracy_score`).
* Построение разделяющей гиперплоскости (`plot`).

### Логистическая регрессия (LogReg1)

 **Класс** : `LogReg1`

Реализация логистической регрессии с использованием сигмоидальной функции. Возможности:

* Предсказания классов и вероятностей (`predict`, `predict_prob`).
* Обучение с использованием градиентного спуска (`fit`).
* Построение границ решений (`plot`).

### Логистическая регрессия (LogReg2)

 **Класс** : `LogReg2`

Альтернативная реализация логистической регрессии с матричными операциями.

Особенности:

* Расчет вероятностей принадлежности объектов к классам.
* Поддержка обучения с динамическим изменением скорости обучения.

### Метод опорных векторов с ядром (SVC2)

 **Класс** : `SVC2`

Реализация SVM с пользовательской ядерной функцией.

* Поддерживает кастомизацию ядра (`K`).
* Использует регуляризацию для предотвращения переобучения.

### Метод ближайших соседей (KNN)

 **Класс** : `KNN`

Метод ближайших соседей с поддержкой различных ядерных функций.

Особенности:

* Различные ядра, такие как `Gaussian`, `Triangular`, `Epanechnikov`, и др.
* Обучение на данных (`fit`).
* Классификация объектов и визуализация (`predict`, `plot`).

---

## Функции и их описание

### Обучение моделей

Каждая модель предоставляет метод `fit` для обучения на тренировочных данных. Пример:

```python
svc = SVC1()
svc.fit(X_train, y_train, a=0.1, n=1000)
```

### Предсказания

Используйте метод `predict` для выполнения классификации:

```python
predictions = svc.predict(X_test)
```

### Оценка качества моделей

Методы `accuracy_score` позволяют оценить точность модели:

```python
accuracy = svc.accuracy_score(y_test, predictions)
```

### Графическое представление данных

Методы `plot` визуализируют разделяющие линии или классификацию.

---

## Как использовать код

1. Импортируйте необходимые классы:
   ```python
   from matplobblib.ml import SVC1, LogReg1, KNN
   ```
2. Подготовьте данные `X` и `y`.
3. Создайте экземпляр модели и обучите её.
4. Выполните предсказание и визуализацию.

---

## Требования к данным

Данные должны быть представлены в виде `pandas.DataFrame` с числовыми значениями. Для визуализации требуется, чтобы признаки были двумерными.

---

## Примеры использования

### Обучение и оценка качества

```python
from matplobblib.ml import SVC1

svc = SVC1()
svc.fit(X_train, y_train)
predictions = svc.predict(X_test)
accuracy = svc.accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy}")
```

### Визуализация результатов

```python
svc.plot(X_train, y_train)
```

---

## Технические детали реализации

### Градиентный спуск

* Используется для обучения моделей (SVC1, LogReg1, LogReg2, SVC2).
* Градиенты вычисляются для минимизации функции потерь.

### Ядерные функции в KNN

* Различные ядра применяются для взвешивания соседей.

---

## Контакты и поддержка

Если у вас есть вопросы, вы можете связаться с автором через GitHub или email.
